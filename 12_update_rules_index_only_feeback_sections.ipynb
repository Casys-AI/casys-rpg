{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nouvelle entrée ajoutée au JSONL :\n",
      "{\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"l'agent de décision doit comprendre les décisions de l'utilisateur en fonction du choix donné par le narrateur\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"Section: 246, l'utilisateur voulait 'je ne suis pas chanceux', l'IA a décidé section 246. Section actuelle:\\n# Section 246\\n\\n— Votre Excellence, bredouillez-vous d'une pauvre voix, je ne pouvais supposer... je veux dire qu'il ne m'est pas venu à l'idée que ce vaisseau pouvait être piloté par l'un des glorieux maîtres du noble Empire arcadien. Mon unique désir est de servir Arcadion. Je croyais avoir affaire à un vaisseau pirate et mon seul but a été de protéger la précieuse cargaison que je transportais. Tentez votre Chance pour savoir si l'Arcadien se laissera abusé par votre histoire. Si vous êtes Chanceux, rendez-vous au [[144]]. Sinon, rendez-vous au [[129]].\\nFeedback: je ne suis pas chanceux, donc je dois aller au 129\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"1. L'IA aurait dû prendre la décision de diriger l'utilisateur vers la section [[129]], car l'utilisateur a clairement indiqué qu'il ne se considère pas chanceux.\\n\\n2. Le feedback est justifié, car selon la logique du jeu, si l'utilisateur ne se sent pas chanceux, il doit suivre le chemin correspondant à cette condition, soit la section [[129]].\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "Section 246 traitée (non incluse dans le JSONL, juste affichée).\n",
      "Feedback feedback_2024-12-13_22-35-26.md traité, fichier déplacé.\n",
      "Nouvelle entrée ajoutée au JSONL :\n",
      "{\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"l'agent de décision doit comprendre les décisions de l'utilisateur en fonction du choix donné par le narrateur\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"Section: 246, l'utilisateur voulait '164', l'IA a décidé section 246. Section actuelle:\\n# Section 246\\n\\n— Votre Excellence, bredouillez-vous d'une pauvre voix, je ne pouvais supposer... je veux dire qu'il ne m'est pas venu à l'idée que ce vaisseau pouvait être piloté par l'un des glorieux maîtres du noble Empire arcadien. Mon unique désir est de servir Arcadion. Je croyais avoir affaire à un vaisseau pirate et mon seul but a été de protéger la précieuse cargaison que je transportais. Tentez votre Chance pour savoir si l'Arcadien se laissera abusé par votre histoire. Si vous êtes Chanceux, rendez-vous au [[144]]. Sinon, rendez-vous au [[129]].\\nFeedback: En fait ici il fallait proposer de lancer le dès pour tenter ma chance et pas autoriser à écrire un numéro (comme je l'ai fait par erreur)\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"1. L'IA aurait dû proposer de lancer le dé pour tenter la chance, conformément à la mécanique de jeu établie dans le texte, sans permettre à l'utilisateur d'entrer un numéro directement.\\n\\n2. Le feedback est justifié, car il souligne une incohérence avec la logique du jeu, où le choix de lancer le dé est une action attendue dans cette situation.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "Section 246 traitée (non incluse dans le JSONL, juste affichée).\n",
      "Feedback feedback_2024-12-13_23-11-42.md traité, fichier déplacé.\n",
      "Sections traitées lors de cette exécution: [246, 246]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import shutil\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import LLMChain\n",
    "\n",
    "# Charger les variables d'environnement\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"La clé API OpenAI n'a pas été trouvée dans le fichier .env\")\n",
    "\n",
    "feedback_dir = \"data/feedback\"\n",
    "processed_dir = \"data/feedback/feedback_processed\"\n",
    "jsonl_path = \"data/decision_examples.jsonl\"\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "def parse_feedback_file(filepath: str):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    section_regex = r\"-\\s*Section actuelle\\s*:\\s*(\\d+)\"\n",
    "    user_response_regex = r\"-\\s*Réponse utilisateur\\s*:\\s*(.*)\"\n",
    "    decision_regex = r\"-\\s*Décision prise\\s*:\\s*Section\\s+(\\d+)\"\n",
    "    section_content_regex = r\"## Section\\s*(.*?)\\s*## Feedback\"\n",
    "    feedback_regex = r\"## Feedback\\s*(.*)\"\n",
    "\n",
    "    section_match = re.search(section_regex, content)\n",
    "    user_response_match = re.search(user_response_regex, content)\n",
    "    decision_match = re.search(decision_regex, content)\n",
    "    section_content_match = re.search(section_content_regex, content, re.DOTALL)\n",
    "    feedback_match = re.search(feedback_regex, content, re.DOTALL)\n",
    "\n",
    "    if not (section_match and user_response_match and decision_match and section_content_match and feedback_match):\n",
    "        print(f\"Impossible de parser le feedback dans {filepath}\")\n",
    "        return None\n",
    "\n",
    "    section = int(section_match.group(1))\n",
    "    user_response = user_response_match.group(1).strip()\n",
    "    taken_decision = int(decision_match.group(1))\n",
    "    section_text = section_content_match.group(1).strip()\n",
    "    feedback_text = feedback_match.group(1).strip()\n",
    "\n",
    "    return {\n",
    "        \"filepath\": filepath,\n",
    "        \"section\": section,\n",
    "        \"user_response\": user_response,\n",
    "        \"taken_decision\": taken_decision,\n",
    "        \"section_text\": section_text,\n",
    "        \"feedback_text\": feedback_text\n",
    "    }\n",
    "\n",
    "def analyze_error_with_langchain(section: int, user_response: str, taken_decision: int, feedback_text: str, section_text: str) -> str:\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"section\", \"user_response\", \"taken_decision\", \"feedback_text\", \"section_text\"],\n",
    "        template=\"\"\"\n",
    "Voici un retour utilisateur sur une décision prise par l'IA dans une histoire interactive.\n",
    "\n",
    "- Section actuelle : {section}\n",
    "- Réponse de l'utilisateur (choix souhaité) : {user_response}\n",
    "- Décision prise par l'IA : {taken_decision}\n",
    "\n",
    "Contenu de la section actuelle :\n",
    "{section_text}\n",
    "\n",
    "Feedback de l'utilisateur sur cette erreur :\n",
    "{feedback_text}\n",
    "\n",
    "Analyse de façon terre à terre et concise :\n",
    "1. Indique quelle décision l'IA aurait dû prendre logiquement.\n",
    "2. Dis si le feedback est justifié en fonction de la logique du jeu.\n",
    "N'évoque aucune émotion, reste purement logique.\n",
    "\"\"\"\n",
    "    )\n",
    "\n",
    "    llm = ChatOpenAI(\n",
    "        temperature=0,\n",
    "        openai_api_key=openai_api_key,\n",
    "        model_name=\"gpt-4o-mini\"\n",
    "    )\n",
    "\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "    analysis = chain.run(\n",
    "        section=section,\n",
    "        user_response=user_response,\n",
    "        taken_decision=taken_decision,\n",
    "        feedback_text=feedback_text,\n",
    "        section_text=section_text\n",
    "    )\n",
    "    return analysis.strip()\n",
    "\n",
    "def append_feedback_to_jsonl(section: int, user_response: str, taken_decision: int, analysis: str, feedback_text: str, section_text: str):\n",
    "    system_content = \"l'agent de décision doit comprendre les décisions de l'utilisateur en fonction du choix donné par le narrateur\"\n",
    "    user_content = (\n",
    "        f\"Section: {section}, l'utilisateur voulait '{user_response}', \"\n",
    "        f\"l'IA a décidé section {taken_decision}. Section actuelle:\\n{section_text}\\n\"\n",
    "        f\"Feedback: {feedback_text}\"\n",
    "    )\n",
    "    assistant_content = analysis\n",
    "\n",
    "    # On ne met pas de type, ni de feedback_section dans le JSONL, seulement messages.\n",
    "    entry = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_content},\n",
    "            {\"role\": \"user\", \"content\": user_content},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    print(\"Nouvelle entrée ajoutée au JSONL :\")\n",
    "    print(json.dumps(entry, ensure_ascii=False, indent=2))\n",
    "\n",
    "    # Écriture dans le JSONL\n",
    "    with open(jsonl_path, 'a', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    # Print la section dans la console sans l'ajouter au JSONL\n",
    "    print(f\"Section {section} traitée (non incluse dans le JSONL, juste affichée).\")\n",
    "\n",
    "def process_all_feedback():\n",
    "    processed_sections = []\n",
    "    files = [f for f in os.listdir(feedback_dir) if f.endswith(\".md\")]\n",
    "    if not files:\n",
    "        print(\"Aucun fichier de feedback à traiter.\")\n",
    "        return processed_sections\n",
    "\n",
    "    for filename in files:\n",
    "        filepath = os.path.join(feedback_dir, filename)\n",
    "        fb = parse_feedback_file(filepath)\n",
    "\n",
    "        if fb is None:\n",
    "            print(f\"Feedback non parsé pour {filename}, fichier laissé en place.\")\n",
    "            continue\n",
    "\n",
    "        analysis = analyze_error_with_langchain(\n",
    "            fb[\"section\"],\n",
    "            fb[\"user_response\"],\n",
    "            fb[\"taken_decision\"],\n",
    "            fb[\"feedback_text\"],\n",
    "            fb[\"section_text\"]\n",
    "        )\n",
    "\n",
    "        append_feedback_to_jsonl(\n",
    "            fb[\"section\"],\n",
    "            fb[\"user_response\"],\n",
    "            fb[\"taken_decision\"],\n",
    "            analysis,\n",
    "            fb[\"feedback_text\"],\n",
    "            fb[\"section_text\"]\n",
    "        )\n",
    "\n",
    "        shutil.move(filepath, os.path.join(processed_dir, filename))\n",
    "        print(f\"Feedback {filename} traité, fichier déplacé.\")\n",
    "        processed_sections.append(fb[\"section\"])\n",
    "\n",
    "    return processed_sections\n",
    "\n",
    "# Exécuter le traitement de tous les feedbacks\n",
    "processed_sections = process_all_feedback()\n",
    "print(\"Sections traitées lors de cette exécution:\", processed_sections)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le fichier JSONL a été modifié après la dernière mise à jour de l'index FAISS.\n",
      "Index FAISS sauvegardé dans data/index/decision_index\\decision_index.faiss.\n",
      "Docstore sauvegardé dans data/index/decision_index\\docstore.json.\n",
      "Les règles de la section 164 ont été enregistrées dans data/rules\\section_164_rule.md\n",
      "Les règles de la section 48 ont été enregistrées dans data/rules\\section_48_rule.md\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import faiss\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from dotenv import load_dotenv\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Charger les variables d'environnement\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"La clé API OpenAI n'a pas été trouvée dans le fichier .env\")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "\n",
    "jsonl_path = 'data/decision_examples.jsonl'\n",
    "sections_dir = 'data/sections'\n",
    "output_directory = 'data/rules'\n",
    "\n",
    "# Dossier dédié à l'index decision_index\n",
    "index_dir = 'data/index/decision_index'\n",
    "faiss_index_path = os.path.join(index_dir, 'decision_index.faiss')\n",
    "docstore_path = os.path.join(index_dir, 'docstore.json')\n",
    "\n",
    "if not os.path.exists(jsonl_path):\n",
    "    raise FileNotFoundError(f\"Le fichier {jsonl_path} n'a pas été trouvé.\")\n",
    "\n",
    "jsonl_mtime = os.path.getmtime(jsonl_path)\n",
    "index_exists = os.path.exists(faiss_index_path) and os.path.exists(docstore_path)\n",
    "\n",
    "recreate_index = False\n",
    "if index_exists:\n",
    "    faiss_mtime = os.path.getmtime(faiss_index_path)\n",
    "    if jsonl_mtime > faiss_mtime:\n",
    "        print(\"Le fichier JSONL a été modifié après la dernière mise à jour de l'index FAISS.\")\n",
    "        recreate_index = True\n",
    "    else:\n",
    "        print(\"L'index FAISS est à jour.\")\n",
    "else:\n",
    "    print(\"L'index FAISS n'existe pas et sera créé.\")\n",
    "    recreate_index = True\n",
    "\n",
    "def create_or_update_index():\n",
    "    data = []\n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                obj = json.loads(line)\n",
    "                data.append(obj)\n",
    "    \n",
    "    texts = []\n",
    "    for entry in data:\n",
    "        if 'text' in entry:\n",
    "            texts.append(entry['text'])\n",
    "        elif 'messages' in entry and isinstance(entry['messages'], list):\n",
    "            combined = \"\\n\".join(msg[\"content\"] for msg in entry[\"messages\"] if \"content\" in msg)\n",
    "            if combined.strip():\n",
    "                texts.append(combined)\n",
    "\n",
    "    if not texts:\n",
    "        print(\"Aucun texte ni messages pour construire l'index.\")\n",
    "        return None, None, None\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    all_chunks = []\n",
    "    for t in texts:\n",
    "        all_chunks.extend(text_splitter.split_text(t))\n",
    "    \n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    embedding_vectors = embeddings.embed_documents(all_chunks)\n",
    "    if not embedding_vectors:\n",
    "        print(\"Aucune embedding générée.\")\n",
    "        return None, None, None\n",
    "\n",
    "    embedding_dim = len(embedding_vectors[0])\n",
    "    \n",
    "    index = faiss.IndexFlatL2(embedding_dim)\n",
    "    index.add(np.array(embedding_vectors).astype('float32'))\n",
    "    \n",
    "    docstore = {str(i): chunk for i, chunk in enumerate(all_chunks)}\n",
    "    \n",
    "    os.makedirs(index_dir, exist_ok=True)\n",
    "    faiss.write_index(index, faiss_index_path)\n",
    "    print(f\"Index FAISS sauvegardé dans {faiss_index_path}.\")\n",
    "    \n",
    "    with open(docstore_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(docstore, f)\n",
    "    print(f\"Docstore sauvegardé dans {docstore_path}.\")\n",
    "    \n",
    "    return index, docstore, embeddings\n",
    "\n",
    "if recreate_index:\n",
    "    index, docstore, embeddings = create_or_update_index()\n",
    "    if index is None:\n",
    "        raise SystemExit(\"Impossible de créer l'index RAG (pas de texte).\")\n",
    "else:\n",
    "    if not (os.path.exists(faiss_index_path) and os.path.exists(docstore_path)):\n",
    "        print(\"Index ou docstore manquant, recréez l'index.\")\n",
    "        raise SystemExit\n",
    "    index = faiss.read_index(faiss_index_path)\n",
    "    with open(docstore_path, 'r', encoding='utf-8') as f:\n",
    "        docstore = json.load(f)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    print(\"Index FAISS et docstore chargés avec succès.\")\n",
    "\n",
    "def process_section(section_number: int):\n",
    "    markdown_path = os.path.join(sections_dir, f\"{section_number}.md\")\n",
    "    if not os.path.exists(markdown_path):\n",
    "        print(f\"La section {section_number} n'existe pas.\")\n",
    "        return\n",
    "\n",
    "    with open(markdown_path, 'r', encoding='utf-8') as f:\n",
    "        section_content = f.read()\n",
    "\n",
    "    query = (\"Donne des exemples pour distinguer l'histoire du livre dont vous êtes le héros \"\n",
    "             \"des règles, choix et décisions purement ludiques. Aide-moi à comprendre, mais \"\n",
    "             \"je n'intégrerai pas ces exemples dans le résultat final.\")\n",
    "    query_embedding = embeddings.embed_query(query)\n",
    "    query_vector = np.array([query_embedding]).astype('float32')\n",
    "\n",
    "    k = 5\n",
    "    distances, indices = index.search(query_vector, k)\n",
    "\n",
    "    retrieved_texts = [docstore[str(i)] for i in indices[0] if str(i) in docstore]\n",
    "    rag_instructions = \"\\n\\n\".join(retrieved_texts)\n",
    "\n",
    "    prompt_template = \"\"\"\n",
    "Tu es un assistant chargé d'extraire les règles du jeu présentées dans une section d'un livre dont vous êtes le héros.\n",
    "Le texte RAG ci-dessous fournit des exemples pour t'aider à comprendre comment séparer l'histoire des vraies règles du jeu.\n",
    "Ne reprends pas ces exemples dans le résultat final, ils sont juste pour t'aider à comprendre.\n",
    "\n",
    "RAG (exemples, ne pas les intégrer dans le résultat) :\n",
    "{rag_instructions}\n",
    "\n",
    "Section :\n",
    "{section_content}\n",
    "\n",
    "Instructions :\n",
    "- Distingue l'histoire (description, narration) des éléments ludiques (choix du joueur, gain/perte de stats, argent, chemins à prendre, combat, énigme...).\n",
    "- N'inclus que les règles et décisions spécifiques à cette section.\n",
    "- Formate les règles en Markdown, par exemple :\n",
    "  - Choix 1: ...\n",
    "  - Endurance, CHance, Habileté +x si ...\n",
    "  - Gagner, perdre de l'argent si\n",
    "  - choisir cette action mène à ...\n",
    "  - devoir se battre contre un monstre si\n",
    "  - si je gagne une partie, je ...\n",
    "  \n",
    "Extrais uniquement ces éléments ludiques propres à cette section, sans reprendre les exemples du RAG.\n",
    "\"\"\"\n",
    "    prompt = PromptTemplate(\n",
    "        template=prompt_template,\n",
    "        input_variables=[\"rag_instructions\", \"section_content\"]\n",
    "    )\n",
    "\n",
    "    llm = ChatOpenAI(\n",
    "        temperature=0,\n",
    "        openai_api_key=openai_api_key,\n",
    "        model_name=\"gpt-4o-mini\"\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "    rules = chain.run(rag_instructions=rag_instructions, section_content=section_content)\n",
    "\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    output_filename = f'section_{section_number}_rule.md'\n",
    "    output_path = os.path.join(output_directory, output_filename)\n",
    "\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(rules)\n",
    "\n",
    "    print(f\"Les règles de la section {section_number} ont été enregistrées dans {output_path}\")\n",
    "\n",
    "#############################\n",
    "# Modes de traitement\n",
    "#############################\n",
    "\n",
    "# Supposons que nous ayons trois modes :\n",
    "# 1. \"new\" : traite uniquement les sections dans processed_sections (viennent d'être traitées par le script précédent)\n",
    "# 2. \"list\" : traite une liste de sections choisies (par exemple [1,48,164])\n",
    "# 3. \"all\" : traite toutes les sections disponibles\n",
    "\n",
    "mode = \"new\"  # Changez le mode selon le besoin\n",
    "\n",
    "# Exemple : récupéré depuis un script précédent\n",
    "processed_sections = [48, 164]  # Sections provenant du script précédent\n",
    "\n",
    "# Liste d'exemples, si mode = \"list\"\n",
    "chosen_sections = [1, 48, 164]\n",
    "\n",
    "if mode == \"new\":\n",
    "    sections_to_process = processed_sections\n",
    "elif mode == \"list\":\n",
    "    sections_to_process = chosen_sections\n",
    "elif mode == \"all\":\n",
    "    all_files = os.listdir(sections_dir)\n",
    "    md_files = [f for f in all_files if f.endswith(\".md\")]\n",
    "    all_sections = []\n",
    "    for mf in md_files:\n",
    "        m = re.search(r'(\\d+)\\.md', mf)\n",
    "        if m:\n",
    "            sec_num = int(m.group(1))\n",
    "            all_sections.append(sec_num)\n",
    "    sections_to_process = sorted(all_sections)\n",
    "else:\n",
    "    sections_to_process = []\n",
    "\n",
    "if isinstance(sections_to_process, int):\n",
    "    sections_to_process = [sections_to_process]\n",
    "\n",
    "# Multi-threading pour traiter les sections\n",
    "num_threads = min(len(sections_to_process), 4)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "    futures = {executor.submit(process_section, sec): sec for sec in sections_to_process}\n",
    "    for future in as_completed(futures):\n",
    "        sec = futures[future]\n",
    "        try:\n",
    "            future.result()\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du traitement de la section {sec}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
