{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aucun fichier de feedback à traiter.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import shutil\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import LLMChain\n",
    "\n",
    "# Charger les variables d'environnement\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"La clé API OpenAI n'a pas été trouvée dans le fichier .env\")\n",
    "\n",
    "feedback_dir = \"data/feedback\"\n",
    "processed_dir = \"data/feedback/feedback_processed\"\n",
    "jsonl_path = \"data/decision_examples.jsonl\"\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "def parse_feedback_file(filepath: str):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    section_regex = r\"-\\s*Section actuelle\\s*:\\s*(\\d+)\"\n",
    "    user_response_regex = r\"-\\s*Réponse utilisateur\\s*:\\s*(.*)\"\n",
    "    decision_regex = r\"-\\s*Décision prise\\s*:\\s*Section\\s+(\\d+)\"\n",
    "    section_content_regex = r\"## Section\\s*(.*?)\\s*## Feedback\"\n",
    "    feedback_regex = r\"## Feedback\\s*(.*)\"\n",
    "\n",
    "    section_match = re.search(section_regex, content)\n",
    "    user_response_match = re.search(user_response_regex, content)\n",
    "    decision_match = re.search(decision_regex, content)\n",
    "    section_content_match = re.search(section_content_regex, content, re.DOTALL)\n",
    "    feedback_match = re.search(feedback_regex, content, re.DOTALL)\n",
    "\n",
    "    if not (section_match and user_response_match and decision_match and section_content_match and feedback_match):\n",
    "        print(f\"Impossible de parser le feedback dans {filepath}\")\n",
    "        return None\n",
    "\n",
    "    section = int(section_match.group(1))\n",
    "    user_response = user_response_match.group(1).strip()\n",
    "    taken_decision = int(decision_match.group(1))\n",
    "    section_text = section_content_match.group(1).strip()\n",
    "    feedback_text = feedback_match.group(1).strip()\n",
    "\n",
    "    return {\n",
    "        \"filepath\": filepath,\n",
    "        \"section\": section,\n",
    "        \"user_response\": user_response,\n",
    "        \"taken_decision\": taken_decision,\n",
    "        \"section_text\": section_text,\n",
    "        \"feedback_text\": feedback_text\n",
    "    }\n",
    "\n",
    "def analyze_error_with_langchain(section: int, user_response: str, taken_decision: int, feedback_text: str, section_text: str) -> str:\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"section\", \"user_response\", \"taken_decision\", \"feedback_text\", \"section_text\"],\n",
    "        template=\"\"\"\n",
    "Voici un retour utilisateur sur une décision prise par l'IA dans une histoire interactive.\n",
    "\n",
    "- Section actuelle : {section}\n",
    "- Réponse de l'utilisateur (choix souhaité) : {user_response}\n",
    "- Décision prise par l'IA : {taken_decision}\n",
    "\n",
    "Contenu de la section actuelle :\n",
    "{section_text}\n",
    "\n",
    "Feedback de l'utilisateur sur cette erreur :\n",
    "{feedback_text}\n",
    "\n",
    "Analyse de façon terre à terre et concise :\n",
    "1. Indique quelle décision l'IA aurait dû prendre logiquement.\n",
    "2. Dis si le feedback est justifié en fonction de la logique du jeu.\n",
    "N'évoque aucune émotion, reste purement logique.\n",
    "\"\"\"\n",
    "    )\n",
    "\n",
    "    llm = ChatOpenAI(\n",
    "        temperature=0,\n",
    "        openai_api_key=openai_api_key,\n",
    "        model_name=\"gpt-4o-mini\"\n",
    "    )\n",
    "\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "    analysis = chain.run(\n",
    "        section=section,\n",
    "        user_response=user_response,\n",
    "        taken_decision=taken_decision,\n",
    "        feedback_text=feedback_text,\n",
    "        section_text=section_text\n",
    "    )\n",
    "    return analysis.strip()\n",
    "\n",
    "def append_feedback_to_jsonl(section: int, user_response: str, taken_decision: int, analysis: str, feedback_text: str, section_text: str):\n",
    "    system_content = \"l'agent de décision doit comprendre les décisions de l'utilisateur en fonction du choix donné par le narrateur\"\n",
    "    user_content = (\n",
    "        f\"Section: {section}, l'utilisateur voulait '{user_response}', \"\n",
    "        f\"l'IA a décidé section {taken_decision}. Section actuelle:\\n{section_text}\\n\"\n",
    "        f\"Feedback: {feedback_text}\"\n",
    "    )\n",
    "    assistant_content = analysis\n",
    "\n",
    "    entry = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_content},\n",
    "            {\"role\": \"user\", \"content\": user_content},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    print(\"Nouvelle entrée ajoutée au JSONL :\")\n",
    "    print(json.dumps(entry, ensure_ascii=False, indent=2))\n",
    "\n",
    "    with open(jsonl_path, 'a', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def process_all_feedback():\n",
    "    files = [f for f in os.listdir(feedback_dir) if f.endswith(\".md\")]\n",
    "    if not files:\n",
    "        print(\"Aucun fichier de feedback à traiter.\")\n",
    "        return\n",
    "\n",
    "    for filename in files:\n",
    "        filepath = os.path.join(feedback_dir, filename)\n",
    "        fb = parse_feedback_file(filepath)\n",
    "\n",
    "        if fb is None:\n",
    "            # On ne déplace pas le fichier s'il n'a pas été parsé\n",
    "            print(f\"Feedback non parsé pour {filename}, fichier laissé en place.\")\n",
    "            continue\n",
    "\n",
    "        analysis = analyze_error_with_langchain(\n",
    "            fb[\"section\"],\n",
    "            fb[\"user_response\"],\n",
    "            fb[\"taken_decision\"],\n",
    "            fb[\"feedback_text\"],\n",
    "            fb[\"section_text\"]\n",
    "        )\n",
    "\n",
    "        append_feedback_to_jsonl(\n",
    "            fb[\"section\"],\n",
    "            fb[\"user_response\"],\n",
    "            fb[\"taken_decision\"],\n",
    "            analysis,\n",
    "            fb[\"feedback_text\"],\n",
    "            fb[\"section_text\"]\n",
    "        )\n",
    "\n",
    "        shutil.move(filepath, os.path.join(processed_dir, filename))\n",
    "        print(f\"Feedback {filename} traité, ajouté au JSONL et fichier déplacé.\")\n",
    "\n",
    "# Exécuter le traitement de tous les feedbacks\n",
    "process_all_feedback()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
